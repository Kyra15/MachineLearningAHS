{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Frame the problem\n",
    "Using the customer description, Define the problem your trying to solve in your own words (remember this is not technial but must be specific so the customer understands the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T12:02:58.723742Z",
     "iopub.status.busy": "2025-10-27T12:02:58.723239Z",
     "iopub.status.idle": "2025-10-27T12:02:58.737007Z",
     "shell.execute_reply": "2025-10-27T12:02:58.735616Z",
     "shell.execute_reply.started": "2025-10-27T12:02:58.723682Z"
    }
   },
   "source": [
    "Using machine learning and deep learning, identify handwritten cursive letters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get the Data \n",
    "Define how you recieved the data (provided, gathered..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python in /home/jupyter-251201/.local/lib/python3.12/site-packages (4.12.0.88)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in /home/jupyter-251201/.local/lib/python3.12/site-packages (from opencv-python) (2.2.6)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "\n",
    "# with zipfile.ZipFile(\"Cursive.zip\", 'r') as zip_ref:\n",
    "#     zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Explore the Data\n",
    "Gain insights into the data you have from step 2, making sure to identify any bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some things I'm thinking about:\n",
    "- black and white\n",
    "- scale images\n",
    "- canny edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import cv2\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique file extensions: {'.heic', '.HEIC', '.JPG', '.jpg', '.jpeg', '.png', '.PNG'}\n"
     ]
    }
   ],
   "source": [
    "all_files = glob.glob('Cursive/**', recursive=True)\n",
    "\n",
    "file_extensions = set()\n",
    "for file_path in all_files:\n",
    "   if os.path.isfile(file_path):\n",
    "       _, extension = os.path.splitext(file_path)\n",
    "       if extension:\n",
    "           file_extensions.add(extension)\n",
    "print(\"Unique file extensions:\", file_extensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".heic: 41\n",
      ".HEIC: 415\n",
      ".JPG: 104\n",
      ".jpg: 106\n",
      ".jpeg: 36\n",
      ".png: 78\n",
      ".PNG: 52\n"
     ]
    }
   ],
   "source": [
    "# look for types of file endings\n",
    "png_count = len(glob.glob('Cursive/**/*.png', recursive=True))\n",
    "\n",
    "for i in file_extensions:\n",
    "    ext_counter = len(glob.glob(f'Cursive/**/*{i}', recursive=True))\n",
    "    print(f\"{i}: {ext_counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# file folder counts\n",
    "dirs = 0\n",
    "files = 0\n",
    "\n",
    "for root, dirnames, filenames in os.walk(\"Cursive\"):\n",
    "    dirs += len(dirnames)\n",
    "    files += len(filenames)\n",
    "\n",
    "print(dirs)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cursive | 1\n",
      "Cursive/S10/S13 | 76\n",
      "Cursive/S11 | 2\n",
      "Cursive/S11/.ipynb_checkpoints | 2\n",
      "Cursive/S14 | 5\n",
      "Cursive/S17 | 25\n",
      "Cursive/S33 | 25\n",
      "Cursive/S34 | 10\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts = Counter()\n",
    "\n",
    "for f in Path(\"Cursive\").rglob(\"*\"):\n",
    "        if f.is_file():\n",
    "            counts[str(f.parent)] += 1\n",
    "\n",
    "bad_folders = []\n",
    "for folder, num_files in sorted(counts.items()):\n",
    "        if num_files != 26:\n",
    "            print(f\"{folder:} | {num_files}\")\n",
    "            bad_folders.append(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to:\n",
    "- Label Data\n",
    "- Crop images (content aware)\n",
    "- Rotate (not sure)\n",
    "- Black and white (opencv)\n",
    "- Canny edge (highlight edges so the letters are easier to pick out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Prepare the Data\n",
    "\n",
    "\n",
    "Apply any data transformations and explain what and why\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cursive/S19', 'Cursive/S35', 'Cursive/S29', 'Cursive/S10', 'Cursive/S10/S11', 'Cursive/S10/S14', 'Cursive/S10/S12', 'Cursive/S26', 'Cursive/S16', 'Cursive/S18', 'Cursive/S20', 'Cursive/S12', 'Cursive/S21', 'Cursive/S2', 'Cursive/S3', 'Cursive/S28', 'Cursive/S32', 'Cursive/S27', 'Cursive/S1', 'Cursive/s15', 'Cursive/S23', 'Cursive/S4', 'Cursive/S30', 'Cursive/S9', 'Cursive/S13', 'Cursive/S5', 'Cursive/S7', 'Cursive/S25', 'Cursive/S31', 'Cursive/S24', 'Cursive/S8', 'Cursive/S6'] 39 32\n"
     ]
    }
   ],
   "source": [
    "# label every handwriting thing as a-z and append index (a_4)\n",
    "# exclude folders with counts != 26\n",
    "\n",
    "all_folders = [f for f in glob.glob('Cursive/**', recursive=True) if os.path.isdir(f)]\n",
    "good_folders = [x for x in all_folders if x not in list(bad_folders) and x != \"Cursive/\"]\n",
    "print(good_folders, len(all_folders), len(good_folders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copied 26 from Cursive/S1\n",
      "copied 26 from Cursive/S2\n",
      "copied 26 from Cursive/S3\n",
      "copied 26 from Cursive/S4\n",
      "copied 26 from Cursive/S5\n",
      "copied 26 from Cursive/S6\n",
      "copied 26 from Cursive/S7\n",
      "copied 26 from Cursive/S8\n",
      "copied 26 from Cursive/S9\n",
      "copied 26 from Cursive/S10\n",
      "no file\n",
      "no file\n",
      "copied 26 from Cursive/S16\n",
      "copied 26 from Cursive/S18\n",
      "copied 26 from Cursive/S19\n",
      "copied 26 from Cursive/S20\n",
      "copied 26 from Cursive/S21\n",
      "copied 26 from Cursive/S23\n",
      "copied 26 from Cursive/S24\n",
      "copied 26 from Cursive/S25\n",
      "copied 26 from Cursive/S26\n",
      "copied 26 from Cursive/S27\n",
      "copied 26 from Cursive/S28\n",
      "copied 26 from Cursive/S29\n",
      "copied 26 from Cursive/S30\n",
      "copied 26 from Cursive/S31\n",
      "copied 26 from Cursive/S32\n",
      "no file\n",
      "copied 26 from Cursive/s15\n",
      "copied 26 from Cursive/S10/S11\n",
      "copied 26 from Cursive/S10/S12\n",
      "copied 26 from Cursive/S10/S14\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import string\n",
    "import shutil\n",
    "\n",
    "\n",
    "letters = string.ascii_lowercase + string.ascii_uppercase\n",
    " \n",
    "src_root = \"\"\n",
    "dst_root = \"LabeledData\"\n",
    "\n",
    "good_folders = sorted(good_folders, key=lambda x: (len(x), x))\n",
    "\n",
    "os.makedirs(dst_root, exist_ok=True)\n",
    "folder_to_label = {folder: letters[i % len(letters)] for i, folder in enumerate(good_folders)}\n",
    "\n",
    "for idx, folder_name in enumerate(good_folders):\n",
    "    src_folder = os.path.join(src_root, folder_name)\n",
    "    dst_folder = os.path.join(dst_root, folder_name)\n",
    "    os.makedirs(dst_folder, exist_ok=True)\n",
    "\n",
    "    files = sorted(\n",
    "        [f for f in glob.glob(os.path.join(src_folder, \"*\"), recursive=True) if os.path.isfile(f)],\n",
    "        key=lambda x: os.path.basename(x)\n",
    "    )\n",
    "\n",
    "    if not files:\n",
    "        print(\"no file\")\n",
    "        continue\n",
    "\n",
    "    folder_label = folder_to_label[folder_name]\n",
    "\n",
    "    for j, path in enumerate(files):\n",
    "        label = letters[j % len(letters)]\n",
    "        ext = os.path.splitext(path)[1]\n",
    "        new_name = f\"{label}_{idx}{ext}\"\n",
    "        dst_path = os.path.join(dst_folder, new_name)\n",
    "        shutil.copy2(path, dst_path)\n",
    "\n",
    "    print(f\"copied {len(files)} from {folder_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pillow_heif\n",
      "  Downloading pillow_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: pillow>=11.1.0 in /opt/tljh/user/lib/python3.12/site-packages (from pillow_heif) (12.0.0)\n",
      "Downloading pillow_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pillow_heif\n",
      "Successfully installed pillow_heif-1.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pillow_heif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path(\"LabeledData/Cursive\")\n",
    "\n",
    "count = 0\n",
    "for checkpoint_dir in root.rglob(\".ipynb_checkpoints\"):\n",
    "    if checkpoint_dir.is_dir():\n",
    "        shutil.rmtree(checkpoint_dir)\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 13\n",
      "saved 1/702 — n_14.jpg\n",
      "saved 27/702 — j_23.PNG\n",
      "saved 53/702 — k_9.JPG\n",
      "saved 79/702 — n_20.HEIC\n",
      "saved 105/702 — x_12.jpeg\n",
      "saved 131/702 — q_13.HEIC\n",
      "saved 157/702 — h_15.png\n",
      "saved 183/702 — t_16.jpg\n",
      "saved 209/702 — f_1.HEIC\n",
      "saved 235/702 — t_2.HEIC\n",
      "saved 261/702 — a_22.PNG\n",
      "saved 287/702 — f_26.png\n",
      "saved 313/702 — h_21.JPG\n",
      "saved 339/702 — u_0.HEIC\n",
      "saved 365/702 — f_28.HEIC\n",
      "saved 391/702 — g_17.HEIC\n",
      "saved 417/702 — q_3.jpg\n",
      "saved 443/702 — w_24.JPG\n",
      "saved 469/702 — y_8.HEIC\n",
      "saved 495/702 — d_4.heic\n",
      "saved 521/702 — x_6.JPG\n",
      "saved 547/702 — u_19.png\n",
      "saved 573/702 — h_25.HEIC\n",
      "saved 599/702 — u_18.HEIC\n",
      "saved 625/702 — r_5.HEIC\n",
      "saved 651/702 — e_29.jpg\n",
      "saved 677/702 — o_31.HEIC\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from pillow_heif import register_heif_opener\n",
    "\n",
    "register_heif_opener()\n",
    "\n",
    "os.makedirs(\"Exploded\", exist_ok=True)\n",
    "\n",
    "formatting = 'png'\n",
    "\n",
    "all_files = list(Path(\"LabeledData/Cursive\").rglob(\"*.*\"))\n",
    "\n",
    "for i, f in enumerate(all_files):\n",
    "    with Image.open(f) as img:\n",
    "        out_path = f\"Exploded/{f.stem}.png\"\n",
    "        img.save(out_path, \"PNG\")\n",
    "    if i % 26 == 0:\n",
    "        print(f\"saved {i+1}/{len(all_files)} — {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrepencies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = list(Path(\"Exploded\").rglob(\"*.*\"))\n",
    "\n",
    "os.makedirs(\"Preprocess\", exist_ok=True)\n",
    "\n",
    "for f in all_the_files:\n",
    "    p = cv2.imread(str(f))\n",
    "    gray = cv2.cvtColor(p, cv2.COLOR_BGR2GRAY)\n",
    "    canny = cv2.Canny(gray, 50, 150)\n",
    "    cv2.imwrite(f'Preprocess/{f.name}', canny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyCAIR in /home/jupyter-251201/.local/lib/python3.12/site-packages (0.1.13)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: natsort in /home/jupyter-251201/.local/lib/python3.12/site-packages (8.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyCAIR\n",
    "!pip install natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyCAIR\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mseam_carve\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cropByColumn\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m seam_img, col_cropped = cropByColumn(image=\u001b[43mimage\u001b[49m, display_seams=\u001b[32m1\u001b[39m, lsit=lsit)\n",
      "\u001b[31mNameError\u001b[39m: name 'image' is not defined"
     ]
    }
   ],
   "source": [
    "from pyCAIR.seam_carve import cropByColumn\n",
    "\n",
    "seam_img, col_cropped = cropByColumn(image=image, display_seams=1, lsit=lsit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale images\n",
    "# https://www.geeksforgeeks.org/python/resize-multiple-images-using-opencv-python/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model the data\n",
    "Using selected ML models, experment with your choices and describe your findings. Finish by selecting a Model to continue with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Fine Tune the Model\n",
    "\n",
    "With the select model descibe the steps taken to acheve the best rusults possiable \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Present\n",
    "In a customer faceing Document provide summery of finding and detail approach taken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Launch the Model System\n",
    "Define your production run code, This should be self susficent and require only your model pramaters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "\n",
    "def infrence(text):\n",
    "\n",
    "    # load everything in\n",
    "    model = joblib.load(\"./models/best_mnb.pkl\")\n",
    "    vectorizer = joblib.load(\"./models/vectorizer.pkl\")\n",
    "    scaler = joblib.load(\"./models/scaler.pkl\")\n",
    "    feature_params = joblib.load(\"./models/feature_params.pkl\")\n",
    "    \n",
    "    s_words = feature_params[0]\n",
    "    h_words = feature_params[1]\n",
    "    good_emails = feature_params[2]\n",
    "\n",
    "    # clean and tokenize the text\n",
    "    cleaned = re.sub(r'[^A-Za-z\\s]', '', text).lower()\n",
    "    tokens = [word for word in cleaned.split() if word not in stopwords and len(word) > 2]\n",
    "    token_counter = collections.Counter(tokens)\n",
    "\n",
    "    # freq of most common words\n",
    "    spam_freq = np.sum([token_counter[word.lower()] for word in s_words])\n",
    "    ham_freq = np.sum([token_counter[word.lower()] for word in h_words])\n",
    "\n",
    "    # amt of punctuation\n",
    "    punctuation_chars = string.punctuation\n",
    "    regex_pat = \"[\" + re.escape(punctuation_chars) + \"]\"\n",
    "    punct_freq = len(re.findall(regex_pat, text))\n",
    "\n",
    "    # good email domain\n",
    "    good_email_count = sum(1 for x in good_emails if x in text.lower())\n",
    "\n",
    "    # msg length\n",
    "    msg_len = len(text)\n",
    "\n",
    "    # create df of numeric\n",
    "    numeric_features = pd.DataFrame([[spam_freq, punct_freq, msg_len, ham_freq, good_email_count]],\n",
    "                                columns=['spam_most_common_freq','punctuation_freq','msg_len','ham_most_common_freq','good_email'])\n",
    "    numeric_features_scaled = scaler.transform(numeric_features)\n",
    "\n",
    "    # vectorize words\n",
    "    text_features = vectorizer.transform([text])\n",
    "    X = hstack([text_features, csr_matrix(numeric_features_scaled)])\n",
    "    \n",
    "    # predcit!\n",
    "    prediction = model.predict(X)\n",
    "        \n",
    "    return prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = '''Subject: stock market information\n",
    "\n",
    "sender : trinity venture , inc . address : 1911 westmead # 2710 , houston , tx 7707 call fax toll free : 877-393 - 7237 hours : 9 5 pm cst hi , receive e-mail address someone interest stock market information . e-mail informational purpose . offer sell solicit security . wish receive type information , please click \" reply \" put \" remove \" subject . permanently remove address our database . news help due diligence abrg , www . yahoo . com click \" stock quote \" . put abrg symbol lookup area click \" quote \" . happy invest ! - dean casia president trinity venture , inc . news release ambra resources group inc . # 610-800 west pender street vancouver , b . c . canada v6c 2v6 symbol abrg ( otc : bb ) acquisition venture oil & gas , inc . ambra resources group inc . adds projects ambra vancouver , british columbium - 4 , 1999 ambra resource group inc . ( otc bb : abrg ) , acquire 50 % ownership venture oil & gas , inc . ambra become 50 % owner property project venture interest . consideration ambra 's purchase 50 % capital stock venture inc . one million shares ambra common stock . part acquisition , venture assign ambra , 50 % interest bastian bay field prospect , state lease \" 9800 \" . 1 , plaquemine parish , louisiana . additional project assign ambra venture ambra shall right request assignment 50 % interest retain venture various project . primary business venture inc . acquire oil gas property re-mediation re-completion work result enhance recovery rate bring back commercial production . current economic condition petroleum industry facilitate acquisition property larger produce company declare surplus property . ambra venture inc . favorable position able acquire property while inventory offering high level . venture currently negotiate acquisition multiple oil gas project texa , louisiana oklahoma , party anticipate add significantly ambra 's resource base productive project . board director john m . hickey , president contact : ambra resource group inc . investor relation : 800-698 - 3377 604-669 - 2723 web site : http : / / www . ambraresource . com release informational purpose . offer sell solicit security product kind . release include information constitute forward-look statement pursuant safe harbor provision private security litigation reform act 1995 . forward-look statement involve risk uncertainty cause actual result differ materially future result encompass within forward-look statement . material provide ambra resource group inc . \" \" basis . ambra resource group inc . expressly disclaim warranty , express imply , include without limitation , warranty merchantability fitness particular purpose , respect service material product . event shall ambra resource group inc . liable direct , indirect , incidental , punitive consequential damages kind whatsoever respect service , material product . trinity venture , inc . receive fee $ 5 , 0 distribute document . permanently remove e-mail address our file call fax us toll free 877-393 - 7237 ; click \" reply \" put \" remove \" subject .\n",
    "'''\n",
    "pred = infrence(test1)\n",
    "print(\"spam\" if pred == 1 else \"ham\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = '''Subject: postdoc position groningen , netherland\n",
    "\n",
    "job position : postdoc dept . social pharmacy pharmacoepidemiology , groningen institute drug study , faculty mathematic natural science , netherland . description work group social pharmacy pharmacoepidemiology perform fundamental apply research epidemiological medical literature database order determine effectiveness / side-effect profile drug . post-doc ask participate program develop computer text analysis pattern recognition technique extraction ( side ) effect profile drug pharmaceutical medical electronic literature database : ( 1 ) source information lead innovative drug research ; ( 2 ) determine benefit-risk profile drug . phd - student assign program . requirement computer linguist computer scientist , complete phd - project expertise corpus linguistics , mathematical linguistics intelligent information retrieval ; interest pharmaceutical science innovative drug research ; expertise datum mine pattern recognition method desire . remark salary basis ministry guideline minimum dfl . 3844 , - maximum dfl . f . 7 . 125 , - ( schaal 10 / 11 rwoo ) bruto pro month , dependent education experience . work group social pharmacy pharmacoepidemiology part dutch school ' groningen - utrecht institute drug exploration ' ( guide ) , acknowledge royal dutch academy science . appointment two . information project : prof . dr . r . vo , email : r . vo @ farm . rug . nl ; tel . + 31 . 50 . 3633331 / 3633272 ; fax . + 31 . 50 . 3633311 . reaction . s . . p . , preferably before july 1st , 1997 . - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - marc weeber http : / / www . farm . rug . nl / marc / home . html groningen university centre pharmacy marc @ farm . rug . nl social pharmacy pharmacoepidemiology tel : + 31 50 3637571 _ _ _ . deusinglaan 2 fax : + 31 50 3633311 | 9713 aw groningen , netherland - - - - - - - - - - - - - - - - - - - - - 0 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "'''\n",
    "pred = infrence(test2)\n",
    "print(\"spam\" if pred == 1 else \"ham\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
